{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ba78ef",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: space-between; background-color: #1e355f; padding: 30px;\">\n",
    "    <div>\n",
    "        <h1 style=\"color: white; text-align: center; font-weight: bold;\">Datos No Estructurados: NLP</h1>\n",
    "        <h2 style=\"color: white; text-align: center;\">Fake News Detection</h2>\n",
    "    </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed137a",
   "metadata": {},
   "source": [
    "<div style=\"padding: 5px;\">\n",
    "    <h3 style=\"color: #1e355f; font-weight: bold;\">Realizado por:</h3>\n",
    "</div>\n",
    "\n",
    "- Álvaro Ezquerro Pérez\n",
    "- María Calvo de Mora Román\n",
    "- Celia Quiles Alemañ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faa122a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b54b632",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: space-between; background-color: #1e355f; padding: 10px;\">\n",
    "    <div>\n",
    "        <h1 style=\"color: white; text-align: center; font-weight: bold;\">Preprocesado de datos para NLP</h1>\n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7120413",
   "metadata": {},
   "source": [
    "El objetivo de este notebook es explicar en profundidad el preprocesado que realizamos posteriormente en todos los notebooks siguientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e332f",
   "metadata": {},
   "source": [
    "### ¿Por qué es necesario el preprocesado para NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c43ea",
   "metadata": {},
   "source": [
    "El preprocesamiento de datos es esencial en el procesamiento del lenguaje natural (NLP) por varias razones clave: [1]\n",
    "\n",
    "1. **Normalización del texto**: El texto en lenguaje natural puede contener una variedad de variaciones y formas, como mayúsculas y minúsculas, abreviaturas, errores tipográficos, etc. Normalizar el texto asegura que esté en un formato consistente para un análisis efectivo.\n",
    "\n",
    "2. **Eliminación de ruido**: El texto puede contener información irrelevante o ruido, como etiquetas HTML, símbolos de puntuación, números, caracteres especiales, etc. Eliminar este ruido puede mejorar la calidad de los datos y hacer que el análisis sea más preciso.\n",
    "\n",
    "3. **Tokenización**: La tokenización implica dividir el texto en unidades más pequeñas, como palabras o subpalabras (tokens). Este paso es fundamental para muchos modelos de NLP, ya que permite al modelo comprender la estructura y el significado del texto.\n",
    "\n",
    "4. **Eliminación de stop words**: Las palabras comunes que no aportan mucho significado al análisis, como \"el\", \"la\", \"a\", \"de\", etc., a menudo se eliminan durante el preprocesamiento para reducir el ruido y mejorar la eficiencia del análisis.\n",
    "\n",
    "5. **Stemming y lematización**: Estos procesos reducen las palabras a sus formas básicas o raíces, lo que ayuda a consolidar el vocabulario y a capturar el significado esencial de las palabras. Por ejemplo, \"corriendo\", \"corrió\" y \"corre\" pueden reducirse todas a \"correr\".\n",
    "\n",
    "6. **Codificación de texto**: Convertir el texto en un formato numérico que los algoritmos de aprendizaje automático puedan entender es fundamental. Esto puede implicar la codificación de palabras como números enteros o vectores, utilizando técnicas como Bag of Words (BoW) o TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "Es decir, el preprocesamiento de datos en NLP es necesario para limpiar y estructurar los datos de texto de manera que los modelos de aprendizaje automático puedan entenderlos y extraer información significativa. Esto no solo mejora la precisión del análisis, sino que también ayuda a reducir el tiempo de procesamiento y mejora la eficiencia del modelo. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789db819",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Como veremos en todos los siguientes notebooks, el preprocesado lo realizaremos de un golpe mediante una función que combina todas las transformaciones que hemos considerado necesarias.\n",
    "\n",
    "Sin embargo, en este notebook mostraremos con un ejemplo, qué hace esta función paso a paso.\n",
    "\n",
    "La función que desglosaremos en la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed6efc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [ps.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f4c78",
   "metadata": {},
   "source": [
    "Para hacer la explicación, usaré el siguiente titular de una Fake New extraída del dataset que estamos empleando en este trabajo:\n",
    "\n",
    "`\"BREAKING NEWS: Vladimir Putin Retaliates After New Sanctions Are Slapped On Russia\".`\n",
    "\n",
    "Consiste en un titular con mayúsculas, nombres propios y signos de puntuación, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9838add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo = \"BREAKING NEWS: Vladimir Putin Retaliates After New Sanctions Are Slapped On Russia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c4fc4",
   "metadata": {},
   "source": [
    "#### 1) Eliminamos caracteres que no son letras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77522ea8",
   "metadata": {},
   "source": [
    "Empezamos eliminando caracteres que no son letras (como números, signos de puntuación, etc.). Así nos aseguramos de que el texto consista solo en palabras relevantes para el análisis, lo que simplifica el procesamiento y mejora la calidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5972a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sin caracteres especiales: BREAKING NEWS  Vladimir Putin Retaliates After New Sanctions Are Slapped On Russia\n"
     ]
    }
   ],
   "source": [
    "texto_limpio = re.sub('[^a-zA-Z]',' ', texto_ejemplo)\n",
    "\n",
    "print(\"Texto sin caracteres especiales:\", texto_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52876b",
   "metadata": {},
   "source": [
    "Como se aprecia en la salida, ya no están los dos puntos (`:`) que sí había antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a0807",
   "metadata": {},
   "source": [
    "#### 2) Convertimos todo el texto a minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb656e9d",
   "metadata": {},
   "source": [
    "En segundo lugar, transformamos las mayúsculas.\n",
    "\n",
    "Al convertir todas las letras del texto a minúsculas, evitamos problemas de sensibilidad a mayúsculas y minúsculas. Esto garantiza que las palabras con la misma raíz, pero escritas de manera diferente, se traten de la misma manera durante el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09574232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto en minúsculas: breaking news  vladimir putin retaliates after new sanctions are slapped on russia\n"
     ]
    }
   ],
   "source": [
    "texto_limpio_minusculas = texto_limpio.lower()\n",
    "\n",
    "print(\"Texto en minúsculas:\", texto_limpio_minusculas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5764028",
   "metadata": {},
   "source": [
    "#### 3) Dividimos en palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe460fc5",
   "metadata": {},
   "source": [
    "Este tercer paso de dividir el texto en un vector de múltiples palabras realmente solo tiene el objetivo de habilitar la posibilidad de realizar las transformaciones de los pasos 4 y 5, las cuales se realizan a nivel de palabra.\n",
    "\n",
    "No obstante, tras esas dos transfromaciones, volveremos a unir todas las palabras en el paso 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c328003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras: ['breaking', 'news', 'vladimir', 'putin', 'retaliates', 'after', 'new', 'sanctions', 'are', 'slapped', 'on', 'russia']\n",
      "Número de palabras: 12\n"
     ]
    }
   ],
   "source": [
    "palabras = texto_limpio_minusculas.split()\n",
    "\n",
    "print(\"Palabras:\", palabras)\n",
    "print(\"Número de palabras:\", len(palabras))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f2fd7d",
   "metadata": {},
   "source": [
    "#### 4) Eliminamos stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd9d1b",
   "metadata": {},
   "source": [
    "Las stopwords son palabras comunes que no aportan mucho significado al análisis, como \"el\", \"la\", \"a\", \"de\", etc. Al eliminar estas palabras, reducimos el ruido en los datos y nos centramos en las palabras más informativas para el análisis.\n",
    "\n",
    "Como los titulares analizados son en inglés, cogemos una lista de stopwords de lengua inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd3db03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras sin stopwords: ['breaking', 'news', 'vladimir', 'putin', 'retaliates', 'new', 'sanctions', 'slapped', 'russia']\n",
      "Número de palabras: 9\n"
     ]
    }
   ],
   "source": [
    "palabras_sin_stopwords = [word for word in palabras if word not in stopwords.words('english')]\n",
    "\n",
    "print(\"Palabras sin stopwords:\", palabras_sin_stopwords)\n",
    "print(\"Número de palabras:\", len(palabras_sin_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f0b1b",
   "metadata": {},
   "source": [
    "Como se aprecia, hemos pasado de tener 12 palabras en el titular, a tener 9. Se han eliminado 'after', 'are' y 'on'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961faf13",
   "metadata": {},
   "source": [
    "#### 5) Aplicamos stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02972aa2",
   "metadata": {},
   "source": [
    "El stemming es el proceso de reducir las palabras a su forma base o raíz. Esto ayuda a consolidar el vocabulario y a capturar el significado esencial de las palabras, lo que facilita el análisis al tratar las variantes de una palabra como una sola entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54d93c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras stemmed: ['break', 'news', 'vladimir', 'putin', 'retali', 'new', 'sanction', 'slap', 'russia']\n"
     ]
    }
   ],
   "source": [
    "palabras_stemmed = [ps.stem(word) for word in palabras_sin_stopwords]\n",
    "\n",
    "print(\"Palabras stemmed:\", palabras_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155895f7",
   "metadata": {},
   "source": [
    "Por ejemplo, 'breaking' ha pasado a ser 'break', 'sanctions' ahora es 'sanction', o 'slapped' es ahora 'slap'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e0a4c",
   "metadata": {},
   "source": [
    "#### 6) Unimos las palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee992d96",
   "metadata": {},
   "source": [
    "Después de aplicar stemming y eliminar stopwords, unimos las palabras nuevamente en un único string. Esto nos da el texto preprocesado y listo para la siguiente etapa del análisis, la vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52b65004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto final: break news vladimir putin retali new sanction slap russia\n"
     ]
    }
   ],
   "source": [
    "texto_final = ' '.join(palabras_stemmed)\n",
    "\n",
    "print(\"Texto final:\", texto_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a5e56",
   "metadata": {},
   "source": [
    "#### 7) Vectorizamos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc0e23b",
   "metadata": {},
   "source": [
    "La vectorización convierte el texto preprocesado en una representación numérica que los algoritmos de aprendizaje automático pueden entender. En el caso de TF-IDF vectorization, se asignan valores numéricos a cada palabra basados en su frecuencia en el documento y en el corpus, lo que permite al modelo entender la importancia relativa de cada palabra en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47ae7fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto vectorizado:   (0, 5)\t0.3333333333333333\n",
      "  (0, 7)\t0.3333333333333333\n",
      "  (0, 6)\t0.3333333333333333\n",
      "  (0, 1)\t0.3333333333333333\n",
      "  (0, 4)\t0.3333333333333333\n",
      "  (0, 3)\t0.3333333333333333\n",
      "  (0, 8)\t0.3333333333333333\n",
      "  (0, 2)\t0.3333333333333333\n",
      "  (0, 0)\t0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos el vectorizador TF-IDF\n",
    "vectorizador = TfidfVectorizer()\n",
    "\n",
    "# aplicamos vectorizador\n",
    "texto_vectorizado = vectorizador.fit_transform([texto_final])\n",
    "\n",
    "print(\"Texto vectorizado:\", texto_vectorizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11674d",
   "metadata": {},
   "source": [
    "Ahora ya sí, tenemos una salida numérica, entendible por el ordenador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956659ee",
   "metadata": {},
   "source": [
    "------\n",
    "Con esto, finalizamos la explicación del preprocesado, que será ejecutado en los siguientes notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f452b57a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<div style=\"padding: 5px;\">\n",
    "    <h2 style=\"color: #1e355f; font-weight: bold;\">Referencias:</h2>\n",
    "</div>\n",
    "\n",
    "\n",
    "[1] ¿Qué es el procesamiento de lenguaje natural? - Explicación del procesamiento de lenguaje natural - AWS. (s. f.). Amazon Web Services, Inc. https://aws.amazon.com/es/what-is/nlp/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
